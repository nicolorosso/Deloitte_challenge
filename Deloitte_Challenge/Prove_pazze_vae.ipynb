{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0037ef26-7b5c-449d-ac95-ecf70543262c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PORVE PAZZE CON VARIATIONAL AUTOECONDERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8bac8b",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9059448-6933-4d4d-a7b5-fd5d496b2bb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bayes_opt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9008/691493102.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbayes_opt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bayes_opt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import yellowbrick\n",
    "import shap\n",
    "import vaex\n",
    "import vaex.ml\n",
    "import datetime as dt\n",
    "from scipy import stats\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, RobustScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error, r2_score, matthews_corrcoef\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import xgboost as xgb\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c68fc6",
   "metadata": {},
   "source": [
    "### IMPORTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "df = pd.read_csv('/content/drive/MyDrive/csv/SupplyChainDataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd5872",
   "metadata": {},
   "source": [
    "### TRASFORMING ORIGINAL DATASET INTO GROUPED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eccf76d6-6b1f-454f-b0f0-fab5ae0a5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_functions = {\n",
    "    'Type': 'first',\n",
    "    'Order Status': 'first',\n",
    "    'Order City': 'first',\n",
    "    'Customer City':'first',\n",
    "    'Order State': 'first',\n",
    "    'Customer State':'first',\n",
    "    'Delivery Status': 'first',\n",
    "    'Shipping Mode':'first',\n",
    "   'Days for shipping (real)': 'first',\n",
    "    'Days for shipping (real)' :'first',\n",
    "    'order date (DateOrders)': 'first',\n",
    "    'Benefit per order': ['sum', 'median'],\n",
    "    'Order Item Discount Rate': 'median',\n",
    "    'Customer Segment': 'first',\n",
    "    'Customer Id': 'first',\n",
    "    'Latitude': 'first',\n",
    "    'Longitude': 'first',\n",
    "    #'Category Id': lambda x: ','.join(x.unique()),\n",
    "    'Category Name': lambda x: ','.join(x.unique()),\n",
    "    # Add the rest of the categorical columns with the lambda function\n",
    "    'Order Item Total': ['sum','median'],\n",
    "    'Order Item Quantity': ['sum']\n",
    "    # Add any additional numerical columns with the appropriate aggregation functions\n",
    "}\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "grouped_df = df.groupby('Order Id').agg(agg_functions).reset_index()\n",
    "grouped_df.columns = ['_'.join(col) if isinstance(col, tuple) else col for col in grouped_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5009d747-8d0e-4798-813d-6348c1166284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "grouped_df['order date (DateOrders)_first'] = pd.to_datetime(grouped_df['order date (DateOrders)_first'])\n",
    "def get_month(x):\n",
    "    return dt.datetime(x.year, x.month, 1)\n",
    "\n",
    "grouped_df['InvoiceMonth'] = grouped_df['order date (DateOrders)_first'].apply(get_month)\n",
    "grouping = grouped_df.groupby('Customer Id_first')['InvoiceMonth']\n",
    "grouped_df['CohortMonth'] = grouping.transform('min')\n",
    "grouping_days = grouped_df.groupby('Customer Id_first')['order date (DateOrders)_first']\n",
    "grouped_df['Cohortdays']  = grouping_days.transform('min')\n",
    "def get_date_int(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    date = df[column].dt.date\n",
    "    hour = df[column].dt.hour\n",
    "    return year, month, date, hour\n",
    "\n",
    "Invoice_Year, Invoice_Month, Invoice_Day, _ = get_date_int(grouped_df, 'order date (DateOrders)_first')\n",
    "Cohort_Year, Cohort_Month , Cohort_day, _ = get_date_int(grouped_df, 'Cohortdays')\n",
    "Year_Diff = Invoice_Year - Cohort_Year\n",
    "Month_Diff = Invoice_Month - Cohort_Month\n",
    "Day_Diff = Invoice_Day - Cohort_day\n",
    "grouped_df['CohortIndex_months'] = Year_Diff*12 + Month_Diff +1\n",
    "grouped_df['CohortIndex_days'] = Day_Diff.dt.days\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "# Convert the comma-separated strings to lists\n",
    "grouped_df['Category Name_<lambda>'] = grouped_df['Category Name_<lambda>'].apply(lambda x: x.split(','))\n",
    "\n",
    "# Initialize MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform the 'Category Name' column\n",
    "category_name_encoded = mlb.fit_transform(grouped_df['Category Name_<lambda>'])\n",
    "\n",
    "# Create a new DataFrame with binary features for each unique item in 'Category Name'\n",
    "category_name_df = pd.DataFrame(category_name_encoded, columns=mlb.classes_)\n",
    "\n",
    "# Combine the new binary features with the original DataFrame\n",
    "grouped_df = pd.concat([grouped_df.drop(columns=['Category Name_<lambda>']), category_name_df], axis=1)\n",
    "\n",
    "grouped_df['order date (DateOrders)_first'] = pd.to_datetime(grouped_df['order date (DateOrders)_first'])\n",
    "grouped_df['order date (DateOrders)_first'] = pd.to_datetime(grouped_df['order date (DateOrders)_first'], unit='ms')\n",
    "grouped_df['Order_Month'] = grouped_df['order date (DateOrders)_first'].dt.month\n",
    "grouped_df['Order_Day'] =grouped_df['order date (DateOrders)_first'].dt.day\n",
    "grouped_df['Order_Weekday'] = grouped_df['order date (DateOrders)_first'].dt.dayofweek\n",
    "\n",
    "grouped_df = vaex.from_pandas(df=grouped_df, copy_index=False)\n",
    "grouped_df = vaex.ml.CycleTransformer(features=['Order_Month'], n=12).fit_transform(grouped_df)\n",
    "grouped_df = vaex.ml.CycleTransformer(features=['Order_Day'], n=31).fit_transform(grouped_df)\n",
    "grouped_df = vaex.ml.CycleTransformer(features=['Order_Weekday'], n=7).fit_transform(grouped_df)\n",
    "grouped_df = grouped_df.to_pandas_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61995997-9487-4b76-8fa5-8fa57af179e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessDataFrame_regression:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def drop_canceled(self):\n",
    "        self.df = self.df[self.df['Order Status_first'] != 'CANCELED']\n",
    "        return self.df\n",
    "  \n",
    "  #def cumulative_orders_customer(self):\n",
    "     # self.df['Customer_Id'] = self.df['Customer_Id'].astype('object')\n",
    "      #self.df['total_number_of_transactions_so_far'] = self.df.groupby('Customer_Id').cumcount() + 1\n",
    "\n",
    "    def process_datetetimes(self):\n",
    "        self.df.order_date_DateOrders = pd.to_datetime(self.df.order_date_DateOrders)\n",
    "        (self.df.order_date_DateOrders.astype(np.int64) / int(1e6)).astype('int64')\n",
    "        return self.df\n",
    "\n",
    "    def augment_with_network_features(self):\n",
    "        self.df['Source'] = ('LA' + self.df['Latitude_first'].astype('str') + '-' + 'LO' + self.df['Longitude_first'].astype('str'))\n",
    "        G = nx.from_pandas_edgelist(self.df, source='Source', target='Order City_first')\n",
    "        self.df['Source_Centrality'] = self.df.Source.map(dict(G.degree))\n",
    "        return self.df\n",
    "\n",
    "\n",
    "\n",
    "#grouped_df = PreprocessDataFrame_regression(grouped_df).add_target_variable()\n",
    "grouped_df = PreprocessDataFrame_regression(grouped_df).drop_canceled()\n",
    "grouped_df = PreprocessDataFrame_regression(grouped_df).augment_with_network_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8321f258-3fcb-45e8-a649-9f7e2415ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = grouped_df.drop(['InvoiceMonth','CohortMonth','Cohortdays', 'order date (DateOrders)_first', 'Order Id_', 'Delivery Status_first', 'Longitude_first', 'Latitude_first'], axis=1)\n",
    "grouped_train, grouped_test = train_test_split(grouped_df, test_size=0.33)\n",
    "grouped_train = vaex.from_pandas(grouped_train)\n",
    "grouped_test = vaex.from_pandas(grouped_test)\n",
    "\n",
    "#grouped train and grouped test\n",
    "grouped_train = vaex.from_pandas(df=grouped_train, copy_index=False)\n",
    "grouped_test = vaex.from_pandas(df=grouped_test, copy_index=False)\n",
    "ohe = vaex.ml.OneHotEncoder(features=[\"Type_first\", \n",
    "                                      \"Customer Segment_first\",'Shipping Mode_first'])\n",
    "grouped_train = ohe.fit_transform(df=grouped_train)\n",
    "grouped_test = ohe.transform(grouped_test)\n",
    "\n",
    "mhe = vaex.ml.MultiHotEncoder(features=[\"Order City_first\", \"Customer City_first\", 'Customer State_first'])\n",
    "grouped_train = mhe.fit_transform(df=grouped_train)\n",
    "grouped_test = mhe.transform(grouped_test)\n",
    "\n",
    "cols_to_scale = ['Days for shipping (real)_first', 'Benefit per order_sum', 'Benefit per order_median',\n",
    "                 'Order Item Discount Rate_median', 'Customer Id_first', 'Order Item Total_sum',\n",
    "                 'Order Item Total_median', 'Order Item Quantity_sum', 'CohortIndex_months',\n",
    "                 'CohortIndex_days', 'Order_Month', 'Order_Day', 'Order_Weekday', 'Order_Month_x',\n",
    "                 'Order_Month_y', 'Order_Day_x', 'Order_Day_y', 'Order_Weekday_x', 'Order_Weekday_y',\n",
    "                 'Source_Centrality']\n",
    "\n",
    "MinMax = vaex.ml.MinMaxScaler(features = cols_to_scale)\n",
    "\n",
    "grouped_train = MinMax.fit_transform(df=grouped_train)\n",
    "grouped_test = MinMax.transform(df=grouped_test)\n",
    "\n",
    "grouped_train = grouped_test.drop(cols_to_scale)\n",
    "grouped_test = grouped_test.drop(cols_to_scale)\n",
    "\n",
    "# Assuming 'grouped_data' contains the preprocessed data with the 'Order Status' column\n",
    "normal_data_train = grouped_train[grouped_train['Order Status_first'] != 'SUSPECTED_FRAUD']\n",
    "anomalous_data_train = grouped_train[grouped_train['Order Status_first'] == 'SUSPECTED_FRAUD']\n",
    "\n",
    "normal_data_test = grouped_test[grouped_test['Order Status_first'] != 'SUSPECTED_FRAUD']\n",
    "anomalous_data_test = grouped_test[grouped_test['Order Status_first'] == 'SUSPECTED_FRAUD']\n",
    "\n",
    "\n",
    "# Drop the 'Order Status' column\n",
    "X_train_normal = normal_data_train.drop(columns=['Order Status_first'])\n",
    "anomalous_data_train = anomalous_data_train.drop(columns=['Order Status_first'])\n",
    "\n",
    "X_test_normal = normal_data_test.drop(columns=['Order Status_first'])\n",
    "anomalous_data_test = anomalous_data_test.drop(columns=['Order Status_first'])\n",
    "\n",
    "X_train_normal = X_train_normal.to_pandas_df()\n",
    "X_test_normal = X_test_normal.to_pandas_df()\n",
    "\n",
    "anomalous_data_train = anomalous_data_train.to_pandas_df()\n",
    "anomalous_data_test = anomalous_data_test.to_pandas_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426bba0a-adca-4656-b0a9-31ab22824aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normal = X_train_normal[X_train_normal.dtypes[X_train_normal.dtypes != 'object'].index.tolist()]\n",
    "X_test_normal = X_test_normal[X_test_normal.dtypes[X_test_normal.dtypes != 'object'].index.tolist()]\n",
    "\n",
    "anomalous_data_train =  anomalous_data_train[anomalous_data_train.dtypes[anomalous_data_train.dtypes != 'object'].index.tolist()]\n",
    "anomalous_data_test =  anomalous_data_train[anomalous_data_train.dtypes[anomalous_data_train.dtypes != 'object'].index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a309a-c0e9-47a0-a4e8-cc33fdb97ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879215d-b268-47fc-b759-e36cbf521676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMAL VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe34b5-f795-4715-b6e9-389a84ef49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense, Dropout\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "input_dim = X_train_normal.shape[1]\n",
    "input_shape = (input_dim,)\n",
    "\n",
    "def build_encoder(input_shape, intermediate_dim, latent_dim):\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(intermediate_dim // 2, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "    return inputs, z_mean, z_log_var\n",
    "\n",
    "def build_decoder(latent_dim, intermediate_dim, output_shape):\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(intermediate_dim // 2, activation='relu')(latent_inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(intermediate_dim, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(output_shape, activation='sigmoid')(x)\n",
    "    return latent_inputs, outputs\n",
    "\n",
    "inputs, z_mean, z_log_var = build_encoder(input_shape, 256, 64)\n",
    "latent_inputs, outputs = build_decoder(64, 256, input_dim)\n",
    "\n",
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sample, output_shape=(64,), name='z')([z_mean, z_log_var])\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    reconstruction_loss = binary_crossentropy(x, x_decoded_mean) * input_dim\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)\n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    return total_loss\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "vae_model.compile(optimizer=opt, loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = vae_model.fit(X_train_normal, X_train_normal,\n",
    "                        shuffle=True,\n",
    "                        epochs=1000,\n",
    "                        batch_size=1000)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1edb0a66-e70d-4c79-84e0-cdaa7a872d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.08003369816332014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "normal_test_reconstructions = vae_model.predict(X_test_normal)\n",
    "anomalous_test_reconstructions = vae_model.predict(anomalous_data_test)\n",
    "\n",
    "normal_test_mse = np.mean(np.power(X_test_normal - normal_test_reconstructions, 2), axis=1)\n",
    "anomalous_test_mse = np.mean(np.power(anomalous_data_test - anomalous_test_reconstructions, 2), axis=1)\n",
    "\n",
    "y_true = np.concatenate([np.zeros(len(normal_test_mse)), np.ones(len(anomalous_test_mse))])\n",
    "y_mse = np.concatenate([normal_test_mse, anomalous_test_mse])\n",
    "\n",
    "thresholds = np.linspace(min(y_mse), max(y_mse), 100)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_mse > threshold).astype('int')\n",
    "    f1_scores.append(f1_score(y_true, y_pred))\n",
    "\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96db7187-5323-4fc0-9ef3-74b3cb0e5a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational Autoencoder:\n",
      "F1-score: 0.045056983832494034\n",
      "Recall: 0.7157894736842105\n",
      "Precision: 0.023260586987753986\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.31      0.47     20773\n",
      "         1.0       0.02      0.72      0.05       475\n",
      "\n",
      "    accuracy                           0.32     21248\n",
      "   macro avg       0.50      0.51      0.26     21248\n",
      "weighted avg       0.96      0.32      0.46     21248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_mse > optimal_threshold).astype('int')\n",
    "\n",
    "print(\"Variational Autoencoder:\")\n",
    "print(\"F1-score:\", f1_score(y_true, y_pred))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a59cf0-78ca-4fe4-b79e-297d73636030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2954c877-b4d4-4ef1-aaaa-bf8dd20afa0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### VAE AUTOENCODERS TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb67eb8-bcb5-40b3-8958-74220bf010f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from vae_tuning\\vae_hyperparameter_tuning\\tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "intermediate_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
      "latent_dim (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 128, 'step': 16, 'sampling': 'linear'}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
      "l2_reg (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.01, 'step': 0.001, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 1e-05, 'conditions': [], 'min_value': 1e-05, 'max_value': 0.001, 'step': None, 'sampling': 'log'}\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`import kerastuner` is deprecated, please use `import keras_tuner`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 112)]             0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 48)                46912     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 112)               36176     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,088\n",
      "Trainable params: 83,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense, Dropout\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from kerastuner import RandomSearch\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "input_dim = X_train_normal.shape[1]\n",
    "input_shape = (input_dim,)\n",
    "\n",
    "def sample(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    intermediate_dim = hp.Int(\"intermediate_dim\", min_value=32, max_value=256, step=32)\n",
    "    latent_dim = hp.Int(\"latent_dim\", min_value=16, max_value=128, step=16)\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.0, max_value=0.5, step=0.1)\n",
    "    l2_reg = hp.Float(\"l2_reg\", min_value=0.0, max_value=0.01, step=0.001)\n",
    "    \n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = Dense(intermediate_dim, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(inputs)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    z_mean = Dense(latent_dim, name='z_mean', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    \n",
    "    \n",
    "    # Lambda layer for reparameterization trick\n",
    "    z = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "    encoder = Model(inputs, z, name='encoder')\n",
    "\n",
    "    # Decoder\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = Dense(intermediate_dim, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))(latent_inputs)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(input_dim, activation='sigmoid', kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    # Full VAE model\n",
    "    outputs = decoder(encoder(inputs))\n",
    "    vae_model = Model(inputs, outputs, name='vae_mlp')\n",
    "    \n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
    "        reconstruction_loss = tf.reduce_sum(tf.math.squared_difference(x, x_decoded_mean))\n",
    "        # compute the KL loss\n",
    "        kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.math.square(z_mean) - tf.math.square(tf.math.exp(z_log_var)), axis=-1)\n",
    "        # return the average loss over all \n",
    "        total_loss = tf.reduce_mean(reconstruction_loss + kl_loss)    \n",
    "        return total_loss\n",
    "\n",
    "    # Compile the model\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"LOG\")\n",
    "    opt = optimizers.Adam(learning_rate=learning_rate)\n",
    "    vae_model.compile(optimizer=opt, loss=vae_loss)\n",
    "    \n",
    "    \n",
    "    return vae_model\n",
    "\n",
    "# Define the Keras Tuner search space\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=2,\n",
    "    directory='vae_tuning',\n",
    "    project_name='vae_hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(X_train_normal, X_train_normal,\n",
    "             epochs=300,\n",
    "             batch_size=1000,\n",
    "             validation_split=0.1)\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ea851ee-ac4f-493a-8803-a87da92550d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model input shape: [(None, 111)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model input shape:\", best_model.layers[0].input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c2461b0-235a-4b1c-bcc6-1a912cd0cd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_normal shape: (20768, 111)\n",
      "anomalous_data_test shape: (480, 111)\n",
      "Input dimension: 111\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test_normal shape:\", X_test_normal.shape)\n",
    "print(\"anomalous_data_test shape:\", anomalous_data_test.shape)\n",
    "print(\"Input dimension:\", input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "baf893b9-3f43-44af-81ee-02a05f158085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "<keras_tuner.engine.hyperparameters.hyperparameters.HyperParameters object at 0x0000026ABDADB6D0>\n",
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 111)]             0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 48)                46688     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 111)               35951     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 82,639\n",
      "Trainable params: 82,639\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "print(\"Best hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0dec451-0e77-44de-8398-bf09e6e6c93a",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [32,111], In[1]: [110,224]\n\t [[{{node dense/Relu}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m decoder \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Encode and reconstruct the normal test data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m z_test_normal \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_normal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m reconstructed_normal \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mpredict(z_test_normal)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Compute the reconstruction error for normal test data\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_v1.py:1058\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1057\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m-> 1058\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:801\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[0;32m    798\u001b[0m x, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_standardize_user_data(\n\u001b[0;32m    799\u001b[0m     x, check_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39msteps\n\u001b[0;32m    800\u001b[0m )\n\u001b[1;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:419\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(\n\u001b[0;32m    415\u001b[0m     mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs\n\u001b[0;32m    416\u001b[0m )\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    421\u001b[0m     batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\backend.py:4577\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4569\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4573\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\n\u001b[0;32m   4574\u001b[0m ):\n\u001b[0;32m   4575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4577\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches) :])\n\u001b[0;32m   4579\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4581\u001b[0m     fetched[: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4582\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4583\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1480\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1481\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1484\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1485\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [32,111], In[1]: [110,224]\n\t [[{{node dense/Relu}}]]"
     ]
    }
   ],
   "source": [
    "# Extract the encoder and decoder models from the best_model\n",
    "encoder = best_model.get_layer('encoder')\n",
    "decoder = best_model.get_layer('decoder')\n",
    "\n",
    "# Encode and reconstruct the normal test data\n",
    "z_test_normal = encoder.predict(X_test_normal)\n",
    "reconstructed_normal = decoder.predict(z_test_normal)\n",
    "\n",
    "# Compute the reconstruction error for normal test data\n",
    "reconstruction_error_normal = np.mean(np.square(X_test_normal - reconstructed_normal), axis=1)\n",
    "\n",
    "# Encode and reconstruct the anomalous test data\n",
    "z_test_fraud = encoder.predict(anomalous_data_test)\n",
    "reconstructed_fraud = decoder.predict(z_test_fraud)\n",
    "\n",
    "# Compute the reconstruction error for anomalous test data\n",
    "reconstruction_error_fraud = np.mean(np.square(anomalous_data_test - reconstructed_fraud), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b523c7-57f3-47d2-bb7e-e351a152e7dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HIERARCHICAL VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49752f2e-9a82-427a-8eab-0a3e9c7866e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hierarchical_vae\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 112)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 56)           6328        ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " z_mean_1 (Dense)               (None, 37)           2109        ['dense_45[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var_1 (Dense)            (None, 37)           2109        ['dense_45[0][0]']               \n",
      "                                                                                                  \n",
      " z_1_sampling (Lambda)          (None, 37)           0           ['z_mean_1[0][0]',               \n",
      "                                                                  'z_log_var_1[0][0]']            \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 56)           2128        ['z_1_sampling[0][0]']           \n",
      "                                                                                                  \n",
      " z_mean_2 (Dense)               (None, 18)           1026        ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var_2 (Dense)            (None, 18)           1026        ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " z_1 (Lambda)                   (None, 37)           0           ['z_mean_1[0][0]',               \n",
      "                                                                  'z_log_var_1[0][0]']            \n",
      "                                                                                                  \n",
      " z_2 (Lambda)                   (None, 18)           0           ['z_mean_2[0][0]',               \n",
      "                                                                  'z_log_var_2[0][0]']            \n",
      "                                                                                                  \n",
      " decoder (Functional)           [(None, 37),         11685       ['z_1[0][0]',                    \n",
      "                                 (None, 112)]                     'z_2[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26,411\n",
      "Trainable params: 26,411\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable <tf.Variable 'dense_47/kernel:0' shape=(18, 56) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m vae_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Finally, we train the model:\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvae_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_normal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_v1.py:855\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    854\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    729\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    730\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    731\u001b[0m         )\n\u001b[0;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:192\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m _update_sample_weight_mode(model, mode, ins)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Get step function and loop type. As part of building the execution\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# function we recompile the metrics based on the updated\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# sample_weight_mode value.\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43m_make_execution_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Prepare validation data. Hold references to the iterator and the input\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# list to properly reinitialize and reuse in multiple validation passes.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m val_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:620\u001b[0m, in \u001b[0;36m_make_execution_function\u001b[1;34m(model, mode)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_distribution_strategy:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distributed_training_utils_v1\u001b[38;5;241m.\u001b[39m_make_execution_function(\n\u001b[0;32m    618\u001b[0m         model, mode\n\u001b[0;32m    619\u001b[0m     )\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_execution_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_v1.py:2363\u001b[0m, in \u001b[0;36mModel._make_execution_function\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m   2361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_execution_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode):\n\u001b[0;32m   2362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ModeKeys\u001b[38;5;241m.\u001b[39mTRAIN:\n\u001b[1;32m-> 2363\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_train_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2364\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function\n\u001b[0;32m   2365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ModeKeys\u001b[38;5;241m.\u001b[39mTEST:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\engine\\training_v1.py:2281\u001b[0m, in \u001b[0;36mModel._make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m   2279\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2280\u001b[0m         \u001b[38;5;66;03m# Training updates\u001b[39;00m\n\u001b[1;32m-> 2281\u001b[0m         updates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_updates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collected_trainable_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2284\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2285\u001b[0m         \u001b[38;5;66;03m# Unconditional updates\u001b[39;00m\n\u001b[0;32m   2286\u001b[0m         updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_updates_for(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:860\u001b[0m, in \u001b[0;36mOptimizerV2.get_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_updates\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, params):\n\u001b[1;32m--> 860\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    861\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, params))\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_valid_dtypes(\n\u001b[0;32m    863\u001b[0m         [\n\u001b[0;32m    864\u001b[0m             v\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m         ]\n\u001b[0;32m    868\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\datavis\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:850\u001b[0m, in \u001b[0;36mOptimizerV2.get_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grad, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grads, params):\n\u001b[0;32m    849\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 850\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    851\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has `None` for gradient. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    852\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that all of your ops have a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    853\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient defined (i.e. are differentiable). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    854\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommon ops without gradient: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    855\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK.argmax, K.round, K.eval.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(param)\n\u001b[0;32m    856\u001b[0m             )\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "\u001b[1;31mValueError\u001b[0m: Variable <tf.Variable 'dense_47/kernel:0' shape=(18, 56) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval."
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "\n",
    "input_dim = X_train_normal.shape[1]\n",
    "input_shape = (input_dim,)\n",
    "intermediate_dim = input_dim // 2\n",
    "\n",
    "\n",
    "def build_hierarchical_encoder(input_shape, intermediate_dim, latent_dim_1, latent_dim_2):\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    # First level\n",
    "    x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "    z_mean_1 = Dense(latent_dim_1, name='z_mean_1')(x)\n",
    "    z_log_var_1 = Dense(latent_dim_1, name='z_log_var_1')(x)\n",
    "    z_1 = Lambda(sample, output_shape=(latent_dim_1,), name='z_1_sampling')([z_mean_1, z_log_var_1])\n",
    "    \n",
    "    # Second level\n",
    "    x = Dense(intermediate_dim, activation='relu')(z_1)\n",
    "    z_mean_2 = Dense(latent_dim_2, name='z_mean_2')(x)\n",
    "    z_log_var_2 = Dense(latent_dim_2, name='z_log_var_2')(x)\n",
    "    z_2 = Lambda(sample, output_shape=(latent_dim_2,), name='z_2_sampling')([z_mean_2, z_log_var_2])\n",
    "    \n",
    "    return inputs, z_mean_1, z_log_var_1, z_mean_2, z_log_var_2\n",
    "\n",
    "\n",
    "def build_hierarchical_decoder(latent_dim_1, latent_dim_2, intermediate_dim, output_shape):\n",
    "    latent_inputs_1 = Input(shape=(latent_dim_1,), name='z_sampling_1')\n",
    "    latent_inputs_2 = Input(shape=(latent_dim_2,), name='z_sampling_2')\n",
    "    \n",
    "    x = Dense(intermediate_dim, activation='relu')(latent_inputs_2)\n",
    "    z_1_decoded = Dense(latent_dim_1, activation='sigmoid')(x)\n",
    "    \n",
    "    x = Dense(intermediate_dim, activation='relu')(latent_inputs_1)\n",
    "    outputs = Dense(output_shape, activation='sigmoid')(x)\n",
    "    \n",
    "    return latent_inputs_1, latent_inputs_2, z_1_decoded, outputs\n",
    "\n",
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def hierarchical_vae_loss(y_true, y_pred):\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = mse(inputs, outputs)\n",
    "    reconstruction_loss *= input_dim\n",
    "\n",
    "    # KL divergence loss for the first level\n",
    "    kl_loss_1 = 1 + z_log_var_1 - K.square(z_mean_1) - K.exp(z_log_var_1)\n",
    "    kl_loss_1 = -0.5 * K.sum(kl_loss_1, axis=-1)\n",
    "\n",
    "    # KL divergence loss for the second level\n",
    "    kl_loss_2 = 1 + z_log_var_2 - K.square(z_mean_2) - K.exp(z_log_var_2)\n",
    "    kl_loss_2 = -0.5 * K.sum(kl_loss_2, axis=-1)\n",
    "\n",
    "    return K.mean(reconstruction_loss + kl_loss_1 + kl_loss_2)\n",
    "\n",
    "\n",
    "latent_dim_1 = int(input_dim / 3)\n",
    "latent_dim_2 = int(latent_dim_1 / 2)\n",
    "\n",
    "inputs, z_mean_1, z_log_var_1, z_mean_2, z_log_var_2 = build_hierarchical_encoder(input_shape, intermediate_dim, latent_dim_1, latent_dim_2)\n",
    "z_1 = Lambda(sample, output_shape=(latent_dim_1,), name='z_1')([z_mean_1, z_log_var_1])\n",
    "z_2 = Lambda(sample, output_shape=(latent_dim_2,), name='z_2')([z_mean_2, z_log_var_2])\n",
    "encoder = Model(inputs, [z_1, z_2], name='encoder')\n",
    "\n",
    "latent_inputs_1, latent_inputs_2, z_1_decoded, outputs = build_hierarchical_decoder(latent_dim_1, latent_dim_2, intermediate_dim, input_dim)\n",
    "decoder = Model([latent_inputs_1, latent_inputs_2], [z_1_decoded, outputs], name='decoder')\n",
    "\n",
    "z_1_decoded, outputs = decoder([z_1, z_2])\n",
    "\n",
    "vae_model = Model(inputs, outputs, name='hierarchical_vae')\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.0001)\n",
    "vae_model.compile(optimizer=opt, loss=hierarchical_vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "# Finally, we train the model:\n",
    "results = vae_model.fit(X_train_normal, X_train_normal,\n",
    "                        shuffle=True,\n",
    "                        epochs=1000,\n",
    "                        batch_size=1000,\n",
    "                        validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
